# -*- coding: utf-8 -*-
"""NLP HONS project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iBnmSVOPEZGzllKpdotat0wtcXPsdku0

# News data sentimentÂ analysis
"""

import pandas as pd

df=pd.read_csv('News Dataset.csv', encoding = "ISO-8859-1")

df.head()

df.shape

df.columns

df.info()

df.isnull().sum()

text_data = []
for row in range(0,len(df.index)):
    text_data.append(' '.join(str(x) for x in df.iloc[row,2:27]))

text_data

data =  {'Text': text_data,'Label':df['Label']}

data = pd.DataFrame(data)

data

import numpy as np
import matplotlib.pyplot as plt
plt.figure(figsize=(7, 3))
plt.bar(data['Label'].value_counts().index, data['Label'].value_counts().values,color=['blue','orange'])
plt.show()

data['Label'].value_counts().index

data['Label'].value_counts().values

import numpy as np
import matplotlib.pyplot as plt
import plotly.graph_objs as ploty
labels = ['1', '0']
values = data['Label'].value_counts()
fig = plt.figure(figsize=(5, 5))
fig=ploty.Figure(data=[ploty.Pie(labels=labels, values=values)])
fig.update_traces(hoverinfo='label+percent', textinfo='value', textfont_size=20,
                  marker=dict(colors=['green', 'orange'],
                              line=dict(color='white',width=3)))
fig.show()

data.replace("[^a-zA-Z]"," ",regex=True, inplace=True)
data.head()

import numpy as np
import re
import nltk
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer


nltk.download("stopwords")
nltk.download("punkt")
nltk.download("wordnet")
nltk.download("omw-1.4")

from nltk.tokenize import word_tokenize
newdata = [word_tokenize(text) for text in text_data]

def preProcessing(text):
    text = text.lower()
    text = text.split()
    lmtzr = WordNetLemmatizer()
    text = [lmtzr.lemmatize(word, 'v') for word in text if not word in set(stopwords.words('english'))]
    text = " ".join(text)
    return text

newdata

y=np.array(data['Label'])

y



from tqdm import tqdm
corpus = []
for index, row in tqdm(data.iterrows()):
    text = preProcessing(row['Text'])
    corpus.append(text)

from nltk.tokenize import word_tokenize
newdata = [word_tokenize(text) for text in corpus]

def build_freqs(newdata, ys):
    # Convert np array to list since zip needs an iterable. The squeeze is necessary or the list ends up with one element.
    yslist = np.squeeze(ys).tolist()
    freqs = {}
    for y, newdata in zip(yslist, newdata):
        for word in newdata:
            pair = (word, y)
            freqs[pair] = freqs.get(pair, 0) + 1
    return freqs

freqs= build_freqs(newdata,y)

freqs

sorted_freqs = sorted(freqs.items(), key=lambda x: (x[0][1], x[1]), reverse=True)

sorted_freqs

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
#from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score
cv = CountVectorizer(ngram_range=(1,2), max_features = 1000)

X = cv.fit_transform(corpus).toarray()
y = data['Label']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3,shuffle = True, random_state = 16)

from sklearn import svm
from sklearn.metrics import classification_report, accuracy_score
model = svm.SVC()

model.fit(X_train, y_train)

yPredicted = model.predict(X_test)

print(f'{classification_report(y_test, yPredicted)} \n\nAccuracy for Support Vector Classifier: {accuracy_score(y_test, yPredicted) * 100} %')

"""###Applying Logistic regression"""

def extract_features(newdata, freqs):
    '''
    Input:
        tweet: a list of words for one tweet
        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)
    Output:
        x: a feature vector of dimension (1,3)
    '''
    word_l = preProcessing(text)
    x = np.zeros((3))

    #bias term is set to 1
    x[0] = 1

    for word in word_l:

        x[1] += freqs.get((word, 1.0),0)
        x[2] += freqs.get((word, 0.0),0)

    return x

tmp1 = extract_features(X_train[0], freqs)
print(tmp1)

X= np.zeros((len(X_train),3))

for i in range(len(X_train)):
    X[i]= extract_features(X_train[i], freqs)
Y = y_train

X

Y.to_numpy()

# Test tweet which does not exits in the freqs dictionary
tmp2 = extract_features('bomb death kill', freqs)
print(tmp2)

def sigmoid(z):
    '''
    Input:
        z: is the input (can be a scalar or an array)
    Output:
        h: the sigmoid of z
    '''
    h = 1 / (1 + np.exp(-z))
    return h

def gradientDescent(x, y, theta, alpha, num_iters):
    '''
    Input:
        x: matrix of features which is (m,n+1)
        y: corresponding labels of the input matrix x, dimensions (m,1)
        theta: weight vector of dimension (n+1,1)
        alpha: learning rate
        num_iters: number of iterations you want to train your model for
    Output:
        J: the final cost
        theta: your final weight vector
    '''
    # get 'm', the number of rows in matrix x
    m = x.shape[0]

    for i in range(0, num_iters):
        z = np.dot(x,theta)
#         print(z)
        h = sigmoid(z)
#         print(h)
        # calculate the cost function
        J = -1./m * (np.dot(y.transpose(), np.log(h)) + np.dot((1-y).transpose(),np.log(1-h)))

        # update the weights theta
        theta= theta - (alpha/m) * np.dot(x.transpose(),(h-y))
#         print(J)
#         print(theta)
#         break
    ### END CODE HERE ###
    J = float(J)
    return J, theta

X.shape

Y=Y.to_numpy()

Y.shape

J, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 1500)
print(f"The cost after training is {J[0, 0]:.8f}.")
print(f"The resulting vector of weights is {[round(t, 8) for t in theta.ravel().tolist()]}")

def predict_text(text, freqs, theta):
    '''
    Input:
        tweet: a string
        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)
        theta: (3,1) vector of weights
    Output:
        y_pred: the probability of a tweet being positive or negative
    '''
    # extract the features of the tweet and store it into x
    x = extract_features(text,freqs)

    # make the prediction using x and theta
    y_pred = sigmoid(np.dot(x,theta))

    return y_pred

# Run this cell to test your function
for text in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:
    print( '%s -> %f' % (text, predict_text(text, freqs, theta)))